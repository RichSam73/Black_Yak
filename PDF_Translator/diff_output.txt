diff --git a/PDF_Translator/app.py b/PDF_Translator/app.py
index 3dc7679..347d99c 100644
--- a/PDF_Translator/app.py
+++ b/PDF_Translator/app.py
@@ -7,9 +7,20 @@ PDF Translator - í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ë‹¤êµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì›¹ì•±
 """
 
 # ë²„ì „ ì •ë³´
-VERSION = "1.7.0"
-VERSION_DATE = "2026-01-09"
+VERSION = "1.8.1"
+VERSION_DATE = "2026-01-11"
 VERSION_NOTES = """
+v1.8.1 (2026-01-11)
+- â˜… ì„±ëŠ¥ ë¡œê·¸ ì¶”ê°€: Batch OCR, Claude API, Gemini Batch, ë³‘ë ¬ ë²ˆì—­ íƒ€ì´ë° ì¶œë ¥
+- â˜… í™•ì • ë²„íŠ¼ í´ë¦­ ì‹œ ë¯¸ë¦¬ë³´ê¸° ì¦‰ì‹œ ê°±ì‹  ë³µêµ¬
+- ë””ë²„ê¹… ë° ì„±ëŠ¥ ë¶„ì„ìš© ìƒì„¸ ë¡œê·¸
+
+v1.8.0 (2026-01-10)
+- â˜… ì‚¬ì „ êµ¬ì¡° í†µí•©: {"í•œê¸€": {"full": "ë²ˆì—­", "abbr": "ì•½ì–´"}} 
+- â˜… UI ì•½ì–´ í¸ì§‘: ìš©ì–´ ì‚¬ì „ì—ì„œ ì•½ì–´ ì§ì ‘ ì¶”ê°€/ìˆ˜ì • ê°€ëŠ¥
+- í•˜ë“œì½”ë”©ëœ ABBREVIATIONS ì œê±°, ì‚¬ì „ ê¸°ë°˜ ì•½ì–´ ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜
+- ì¥ê¸°ì  í™•ì¥ì„± ê°œì„  (category, note ë“± í•„ë“œ ì¶”ê°€ ìš©ì´)
+
 v1.7.0 (2026-01-09)
 - â˜… ìš©ì–´ ì‚¬ì „ ê´€ë¦¬ ê¸°ëŠ¥: ì˜ë¥˜ ì „ë¬¸ ìš©ì–´ ì¶”ê°€/ìˆ˜ì •/ì‚­ì œ ê°€ëŠ¥
 - â˜… ì‚¬ì „ í›„ì²˜ë¦¬: AI ë²ˆì—­ í›„ ì‚¬ì „ ìš©ì–´ë¡œ ìë™ êµì • (ì¼ê´€ì„± í–¥ìƒ)
@@ -74,12 +85,14 @@ import requests
 import logging
 from collections import Counter
 from datetime import datetime
+from concurrent.futures import ThreadPoolExecutor, as_completed  # ë³‘ë ¬ ì²˜ë¦¬ìš©
 from flask import Flask, render_template_string, request, send_file, jsonify
 from PIL import Image, ImageDraw, ImageFont
 import numpy as np
 from paddleocr import PaddleOCR
 import cv2
 import fitz  # PyMuPDF
+from img2table.document import Image as Img2TableImage  # í…Œì´ë¸” ê°ì§€ìš©
 
 # â˜… ë¡œê¹… ì„¤ì • (ê²¹ì¹¨ ê°ì§€ ë””ë²„ê¹…ìš©)
 LOG_FILE = os.path.join(os.path.dirname(__file__), 'overlap_debug.log')
@@ -167,22 +180,22 @@ LANGUAGE_CONFIG = {
     }
 }
 
-# ì•½ì–´ ë§¤í•‘ ì‚¬ì „ (ê¸´ í…ìŠ¤íŠ¸ â†’ ì§§ì€ ì•½ì–´)
-ABBREVIATIONS = {
-    "Garment Matching": "G.M",
-    "G Matching": "G.M",
-    "Accessory Matching": "A.M",
-    "A Matching": "A.M",
-    "Consumption": "Cons.",
-    "NaturalZipper": "Nat.Zip",
-    "Natural Zipper": "Nat.Zip",
-    "FrontZipper": "Fr.Zip",
-    "Front Zipper": "Fr.Zip",
-    "SidePocket": "Side Pkt",
-    "Side Pocket": "Side Pkt",
-    "Factory Handling": "Fact.Hdl",
-    "Hood/Hem": "Hd/Hm",
-}
+# [ë ˆê±°ì‹œ] í•˜ë“œì½”ë”©ëœ ì•½ì–´ - ì´ì œ garment_dict.jsonì˜ abbr í•„ë“œë¡œ ëŒ€ì²´ë¨
+# ABBREVIATIONS = {
+#     "Garment Matching": "G.M",
+#     "G Matching": "G.M",
+#     "Accessory Matching": "A.M",
+#     "A Matching": "A.M",
+#     "Consumption": "Cons.",
+#     "NaturalZipper": "Nat.Zip",
+#     "Natural Zipper": "Nat.Zip",
+#     "FrontZipper": "Fr.Zip",
+#     "Front Zipper": "Fr.Zip",
+#     "SidePocket": "Side Pkt",
+#     "Side Pocket": "Side Pkt",
+#     "Factory Handling": "Fact.Hdl",
+#     "Hood/Hem": "Hd/Hm",
+# }
 
 # ì˜ë¥˜ ì „ë¬¸ ìš©ì–´ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
 GARMENT_DICT_FILE = os.path.join(os.path.dirname(__file__), "garment_dict.json")
@@ -305,6 +318,81 @@ def get_ocr_results(image_path):
     return texts
 
 
+def get_ocr_results_batch(image_paths):
+    """ë°°ì¹˜ OCR - ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ í•œë²ˆì— ì²˜ë¦¬ (ì†ë„ í–¥ìƒ)
+    
+    Args:
+        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
+        
+    Returns:
+        list: ê° ì´ë¯¸ì§€ë³„ OCR ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
+    """
+    import time
+    batch_start = time.time()
+    
+    ocr = get_ocr_engine()
+    
+    # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ RGB numpy ë°°ì—´ë¡œ ë³€í™˜
+    load_start = time.time()
+    images_rgb = []
+    for img_path in image_paths:
+        img_bgr = cv2.imread(img_path)
+        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
+        images_rgb.append(img_rgb)
+    load_time = time.time() - load_start
+    print(f"[Batch OCR] Image loading: {load_time:.2f}s for {len(images_rgb)} images", flush=True)
+    
+    # ë°°ì¹˜ OCR ì‹¤í–‰
+    ocr_start = time.time()
+    print(f"[Batch OCR] Running OCR on {len(images_rgb)} images...", flush=True)
+    results = ocr.predict(images_rgb)
+    ocr_time = time.time() - ocr_start
+    print(f"[Batch OCR] OCR inference: {ocr_time:.2f}s", flush=True)
+    
+    # ê²°ê³¼ íŒŒì‹±
+    all_texts = []
+    for page_idx, result in enumerate(results if results else []):
+        texts = []
+        rec_texts = []
+        rec_scores = []
+        dt_polys = []
+        
+        # OCRResult ê°ì²´ ì²˜ë¦¬
+        if hasattr(result, 'rec_texts'):
+            rec_texts = result.rec_texts or []
+            rec_scores = result.rec_scores or []
+            dt_polys = result.dt_polys if hasattr(result, 'dt_polys') and result.dt_polys is not None else []
+        elif isinstance(result, dict):
+            rec_texts = result.get('rec_text', result.get('rec_texts', []))
+            rec_scores = result.get('rec_score', result.get('rec_scores', []))
+            dt_polys = result.get('dt_polys', [])
+        
+        if isinstance(rec_texts, str):
+            rec_texts = [rec_texts]
+            rec_scores = [rec_scores]
+            dt_polys = [dt_polys]
+        
+        for text, score, poly in zip(rec_texts, rec_scores, dt_polys):
+            text_str = str(text)
+            has_korean = any('\uac00' <= c <= '\ud7a3' for c in text_str)
+            bbox = poly.tolist() if hasattr(poly, 'tolist') else poly
+            texts.append({
+                "bbox": bbox,
+                "text": text_str,
+                "confidence": float(score) if score else 1.0,
+                "has_korean": has_korean
+            })
+        
+        all_texts.append(texts)
+        print(f"  [Page {page_idx+1}] Found {len(texts)} texts", flush=True)
+    
+    total_time = time.time() - batch_start
+    total_texts = sum(len(t) for t in all_texts)
+    print(f"[Batch OCR] TOTAL: {total_time:.2f}s for {len(image_paths)} pages, {total_texts} texts", flush=True)
+    
+    return all_texts
+
+
 def translate_with_dict(korean_text, target_lang):
     """ì‚¬ì „ ê¸°ë°˜ ë²ˆì—­ (fallbackìš©)"""
     result = korean_text
@@ -338,16 +426,88 @@ def apply_dict_preprocess(korean_text, target_lang):
     sorted_terms = sorted(dict_terms.items(), key=lambda x: len(x[0]), reverse=True)
 
     term_idx = 1
-    for korean_term, translation in sorted_terms:
+    for korean_term, term_data in sorted_terms:
         if korean_term in result:
             placeholder = f"<<TERM_{term_idx}>>"
             result = result.replace(korean_term, placeholder)
-            placeholder_map[placeholder] = translation
+            # ìƒˆ êµ¬ì¡°: term_data = {"full": "ë²ˆì—­", "abbr": "ì•½ì–´"}
+            if isinstance(term_data, dict):
+                placeholder_map[placeholder] = term_data.get("full", "")
+            else:
+                # ë ˆê±°ì‹œ í˜¸í™˜: ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš°
+                placeholder_map[placeholder] = term_data
             term_idx += 1
 
     return result, placeholder_map
 
 
+def detect_table_regions(image_path, max_avg_row_height=50):
+    """img2tableì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ì˜ì—­ ê°ì§€
+    
+    Args:
+        image_path: ì´ë¯¸ì§€ ê²½ë¡œ
+        max_avg_row_height: í…Œì´ë¸”ë¡œ ì¸ì •í•  ìµœëŒ€ í‰ê·  í–‰ ë†’ì´ (ê¸°ë³¸ 50px)
+    
+    Returns:
+        list: í…Œì´ë¸” ì˜ì—­ bbox ë¦¬ìŠ¤íŠ¸ [(x1, y1, x2, y2), ...]
+    """
+    try:
+        img = Img2TableImage(src=image_path)
+        tables = img.extract_tables()
+        logger.info(f"[Table Detection] Found {len(tables)} raw tables")
+        
+        table_regions = []
+        for idx, table in enumerate(tables):
+            # í–‰ ë†’ì´ ê³„ì‚°
+            if hasattr(table, 'content') and table.content:
+                row_heights = []
+                for row in table.content:
+                    if row:
+                        for cell in row:
+                            if cell and hasattr(cell, 'bbox'):
+                                cell_bbox = cell.bbox
+                                if hasattr(cell_bbox, 'y1') and hasattr(cell_bbox, 'y2'):
+                                    row_heights.append(cell_bbox.y2 - cell_bbox.y1)
+                                break
+                
+                if row_heights:
+                    avg_row_height = sum(row_heights) / len(row_heights)
+                    logger.info(f"[Table Detection] Table #{idx} avg row height: {avg_row_height:.1f}px")
+                    if avg_row_height > max_avg_row_height:
+                        logger.info(f"[Table Detection] Table #{idx} skipped (height > {max_avg_row_height}px)")
+                        continue
+            
+            # bbox ì¶”ì¶œ
+            if hasattr(table, 'bbox'):
+                bbox = table.bbox
+                if hasattr(bbox, 'x1'):
+                    table_regions.append((bbox.x1, bbox.y1, bbox.x2, bbox.y2))
+                    logger.info(f"[Table Detection] Table #{idx} added: ({bbox.x1}, {bbox.y1}, {bbox.x2}, {bbox.y2})")
+        
+        logger.info(f"[Table Detection] Final: {len(table_regions)} valid tables")
+        return table_regions
+    except Exception as e:
+        logger.error(f"[Table Detection] Error: {e}")
+        return []
+
+
+def is_inside_table(bbox, table_regions):
+    """í…ìŠ¤íŠ¸ bboxê°€ í…Œì´ë¸” ì˜ì—­ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸"""
+    if not table_regions:
+        return False
+    
+    # bbox ì¤‘ì‹¬ì  ê³„ì‚°
+    xs = [p[0] for p in bbox]
+    ys = [p[1] for p in bbox]
+    center_x = (min(xs) + max(xs)) / 2
+    center_y = (min(ys) + max(ys)) / 2
+    
+    for (tx1, ty1, tx2, ty2) in table_regions:
+        if tx1 <= center_x <= tx2 and ty1 <= center_y <= ty2:
+            return True
+    return False
+
+
 def restore_placeholders(translated_text, placeholder_map):
     """ë²ˆì—­ ê²°ê³¼ì—ì„œ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ì „ ë²ˆì—­ìœ¼ë¡œ ë³µì›
 
@@ -362,9 +522,46 @@ def restore_placeholders(translated_text, placeholder_map):
         ("23SS Hanger Loop <<TERM_1>>", {"<<TERM_1>>": "Consumption"})
         â†’ "23SS Hanger Loop Consumption"
     """
+    import re
     result = translated_text
+    
     for placeholder, translation in placeholder_map.items():
+        # ì›ë³¸ placeholder (ì˜ˆ: <<TERM_1>>)
         result = result.replace(placeholder, translation)
+        
+        # AIê°€ ë³€í˜•í•œ ë‹¤ì–‘í•œ íŒ¨í„´ë„ ì²˜ë¦¬
+        # <<TERM_1>> ì—ì„œ ìˆ«ì ì¶”ì¶œ
+        match = re.search(r'TERM_(\d+)', placeholder)
+        if match:
+            term_num = match.group(1)
+            # ë‹¤ì–‘í•œ ë³€í˜• íŒ¨í„´ ì²˜ë¦¬ (ì •í™•í•œ ë¬¸ìì—´ ë§¤ì¹­)
+            variations = [
+                f"TERM_{term_num}",           # TERM_1 (êº¾ì‡  ì œê±°ë¨)
+                f"TERM {term_num}",           # TERM 1 (ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°ë¨)
+                f"<TERM_{term_num}>",         # <TERM_1> (êº¾ì‡  í•˜ë‚˜ë§Œ)
+                f"[TERM_{term_num}]",         # [TERM_1] (ëŒ€ê´„í˜¸ë¡œ ë³€í˜•)
+                f"(TERM_{term_num})",         # (TERM_1) (ê´„í˜¸ë¡œ ë³€í˜•)
+                f"{{TERM_{term_num}}}",       # {TERM_1} (ì¤‘ê´„í˜¸ë¡œ ë³€í˜•)
+                f"TERM{term_num}",            # TERM1 (ì–¸ë”ìŠ¤ì½”ì–´ ì™„ì „ ì œê±°)
+                f"Term_{term_num}",           # Term_1 (ëŒ€ì†Œë¬¸ì ë³€í˜•)
+                f"term_{term_num}",           # term_1 (ì†Œë¬¸ì ë³€í˜•)
+            ]
+            for var in variations:
+                if var in result:
+                    result = result.replace(var, translation)
+            
+            # ì •ê·œì‹ìœ¼ë¡œ ë” ìœ ì—°í•œ íŒ¨í„´ ë§¤ì¹­ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
+            # ì˜ˆ: "TERM _ 1", "TERM- 1", "TERM_1." ë“±
+            flexible_patterns = [
+                rf'<<\s*TERM[_\s-]*{term_num}\s*>>',  # << TERM_1 >> ë“±
+                rf'<\s*TERM[_\s-]*{term_num}\s*>',    # < TERM_1 > ë“±
+                rf'\[\s*TERM[_\s-]*{term_num}\s*\]',  # [ TERM_1 ] ë“±
+                rf'\(\s*TERM[_\s-]*{term_num}\s*\)',  # ( TERM_1 ) ë“±
+                rf'TERM[_\s-]*{term_num}(?![0-9])',   # TERM_1, TERM 1, TERM-1 (ë’¤ì— ìˆ«ì ì—†ì„ ë•Œë§Œ)
+            ]
+            for pattern in flexible_patterns:
+                result = re.sub(pattern, translation, result, flags=re.IGNORECASE)
+    
     return result
 
 
@@ -396,6 +593,9 @@ def apply_dict_postprocess(translated_text, original_korean, target_lang):
 
 def translate_with_claude(image_path, texts, target_lang, api_key, model=None):
     """Claude APIë¡œ ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë²ˆì—­ (Placeholder ë°©ì‹ ì ìš©)"""
+    import time
+    api_start = time.time()
+    
     print(f"[Claude] translate_with_claude called - texts: {len(texts)}, model: {model}", flush=True)
     if model is None:
         model = AI_MODELS["claude"]["default"]
@@ -475,13 +675,15 @@ Korean texts:
         }
 
         print(f"[Claude] Calling API: {CLAUDE_API_URL}", flush=True)
+        request_start = time.time()
         response = requests.post(
             CLAUDE_API_URL,
             headers=headers,
             json=payload,
             timeout=120
         )
-        print(f"[Claude] API response status: {response.status_code}", flush=True)
+        api_time = time.time() - request_start
+        print(f"[Claude] API response status: {response.status_code} (took {api_time:.2f}s)", flush=True)
 
         if response.status_code == 200:
             result = response.json()
@@ -547,6 +749,8 @@ Korean texts:
                 translated = item["text"]  # ì˜ì–´ ì›ë³¸ ìœ ì§€
             translations.append({**item, "translated": translated})
 
+    total_time = time.time() - api_start
+    print(f"[Claude] TOTAL: {total_time:.2f}s for {len(texts)} texts ({len(korean_list)} Korean)", flush=True)
     return translations
 
 
@@ -701,9 +905,16 @@ def translate_batch_with_gemini(all_pages_texts, target_lang, api_key, model=Non
     Returns:
         {page_idx: [translated_texts], ...}
     """
+    import time
+    batch_start = time.time()
+    
     if model is None:
         model = AI_MODELS["gemini"]["default"]
     lang_config = LANGUAGE_CONFIG.get(target_lang, LANGUAGE_CONFIG["english"])
+    
+    total_pages = len(all_pages_texts)
+    total_texts = sum(len(p["texts"]) for p in all_pages_texts)
+    print(f"[Gemini Batch] Starting batch translation: {total_pages} pages, {total_texts} texts", flush=True)
 
     # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹¨ (í˜ì´ì§€ êµ¬ë¶„ í¬í•¨)
     all_korean = []
@@ -801,6 +1012,8 @@ Korean texts:
 
                 result_by_page[page_idx] = page_translations
 
+            total_time = time.time() - batch_start
+            print(f"[Gemini Batch] TOTAL: {total_time:.2f}s for {total_pages} pages, {total_texts} texts (1 API call)", flush=True)
             return result_by_page
         else:
             print(f"Gemini Batch API error: {response.status_code} - {response.text}", flush=True)
@@ -949,6 +1162,66 @@ Korean texts:
     return translations
 
 
+def translate_pages_parallel(pages_data, target_lang, ai_engine, api_key, model, max_workers=3):
+    """ë³‘ë ¬ ë²ˆì—­ - ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ë™ì‹œì— ë²ˆì—­ (Claude, OpenAIìš©)
+    
+    Args:
+        pages_data: [{"page_idx": 0, "img_path": "...", "texts": [...]}, ...]
+        target_lang: ëŒ€ìƒ ì–¸ì–´
+        ai_engine: AI ì—”ì§„ (claude, openai)
+        api_key: API í‚¤
+        model: ëª¨ë¸ëª…
+        max_workers: ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ 3, API rate limit ê³ ë ¤)
+        
+    Returns:
+        dict: {page_idx: translations, ...}
+    """
+    import time
+    parallel_start = time.time()
+    results = {}
+    page_times = {}  # ê° í˜ì´ì§€ë³„ ì†Œìš” ì‹œê°„
+    
+    def translate_single_page(page_data):
+        """ë‹¨ì¼ í˜ì´ì§€ ë²ˆì—­ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
+        page_start = time.time()
+        page_idx = page_data["page_idx"]
+        img_path = page_data["img_path"]
+        texts = page_data["texts"]
+        
+        if not texts:
+            return page_idx, [], 0
+        
+        try:
+            translations = translate_with_vlm(img_path, texts, target_lang, ai_engine, api_key, model)
+            elapsed = time.time() - page_start
+            print(f"  [Parallel] Page {page_idx+1} done - {len(translations)} texts in {elapsed:.2f}s", flush=True)
+            return page_idx, translations, elapsed
+        except Exception as e:
+            elapsed = time.time() - page_start
+            print(f"  [Parallel] Page {page_idx+1} ERROR in {elapsed:.2f}s: {e}", flush=True)
+            # ì—ëŸ¬ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
+            return page_idx, [{"bbox": t["bbox"], "text": t["text"], "translated": t["text"], 
+                             "has_korean": t.get("has_korean", True)} for t in texts], elapsed
+    
+    total_texts = sum(len(p["texts"]) for p in pages_data)
+    print(f"[Parallel Translation] Starting {len(pages_data)} pages ({total_texts} texts) with {max_workers} workers...", flush=True)
+    
+    with ThreadPoolExecutor(max_workers=max_workers) as executor:
+        # ëª¨ë“  í˜ì´ì§€ ë²ˆì—­ ì‘ì—… ì œì¶œ
+        futures = {executor.submit(translate_single_page, pd): pd["page_idx"] for pd in pages_data}
+        
+        # ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
+        for future in as_completed(futures):
+            page_idx, translations, elapsed = future.result()
+            results[page_idx] = translations
+            page_times[page_idx] = elapsed
+    
+    total_time = time.time() - parallel_start
+    avg_time = sum(page_times.values()) / len(page_times) if page_times else 0
+    print(f"[Parallel Translation] TOTAL: {total_time:.2f}s (avg per page: {avg_time:.2f}s, workers: {max_workers})", flush=True)
+    return results
+
+
 def translate_with_vlm(image_path, texts, target_lang, ai_engine="ollama", api_key=None, model=None):
     """VLMìœ¼ë¡œ ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë²ˆì—­ (Ollama, Claude, GPT-4, Gemini)"""
 
@@ -1264,21 +1537,30 @@ def check_bbox_overlap(bbox1, bbox2):
     return True
 
 
-def abbreviate_text(text, used_abbreviations):
-    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì•½ì–´ë¡œ ì¶•ì•½
+def abbreviate_text(text, used_abbreviations, target_lang="english"):
+    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì•½ì–´ë¡œ ì¶•ì•½ (ì‚¬ì „ ê¸°ë°˜)
 
     Args:
         text: ì›ë³¸ í…ìŠ¤íŠ¸
         used_abbreviations: ì‚¬ìš©ëœ ì•½ì–´ ì¶”ì ìš© set (ìˆ˜ì •ë¨)
+        target_lang: ëŒ€ìƒ ì–¸ì–´ (ì‚¬ì „ì—ì„œ ì•½ì–´ ì¡°íšŒìš©)
 
     Returns:
         str: ì¶•ì•½ëœ í…ìŠ¤íŠ¸
     """
     result = text
-    for full_text, abbr in ABBREVIATIONS.items():
-        if full_text in result:
-            result = result.replace(full_text, abbr)
-            used_abbreviations.add((abbr, full_text))  # (ì•½ì–´, ì›ë¬¸) ì €ì¥
+    
+    # ì‚¬ì „ì—ì„œ ì•½ì–´ ì¡°íšŒ
+    if target_lang in GARMENT_DICT:
+        lang_dict = GARMENT_DICT[target_lang]
+        for korean_term, term_data in lang_dict.items():
+            if isinstance(term_data, dict):
+                full_text = term_data.get("full", "")
+                abbr = term_data.get("abbr", "")
+                if abbr and full_text in result:
+                    result = result.replace(full_text, abbr)
+                    used_abbreviations.add((abbr, full_text))  # (ì•½ì–´, ì›ë¬¸) ì €ì¥
+    
     return result
 
 
@@ -1385,10 +1667,13 @@ def draw_vertical_text(draw, text, x, y, font, fill, box_width, box_height):
         current_y += char_height
 
 
-def replace_text_in_image(image_path, translations, output_path):
+def replace_text_in_image(image_path, translations, output_path, target_lang="english"):
     """ì´ë¯¸ì§€ì—ì„œ í•œê¸€ ì˜ì—­ì„ ì§€ìš°ê³  ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¡œ êµì²´ - v1.8.2 (ì˜ì–´ í…ìŠ¤íŠ¸ ìœ ì§€, ê²¹ì¹¨ ê°ì§€ìš© í¬í•¨)"""
     img = cv2.imread(image_path)
     height, width = img.shape[:2]
+    
+    # â˜… í…Œì´ë¸” ì˜ì—­ ê°ì§€ (ì¤‘ì•™ ì •ë ¬ ì ìš© ì—¬ë¶€ íŒë‹¨ìš©)
+    table_regions = detect_table_regions(image_path)
 
     # 1ë‹¨ê³„: í•œê¸€ í…ìŠ¤íŠ¸ ì˜ì—­ë§Œ ë°°ê²½ìƒ‰ìœ¼ë¡œ ì§€ìš°ê¸° (ì˜ì–´ëŠ” ì›ë³¸ ìœ ì§€)
     bg_colors = {}
@@ -1454,6 +1739,8 @@ def replace_text_in_image(image_path, translations, output_path):
         is_vertical = is_vertical_text(bbox)
         # ê²¹ì¹¨ ê°ì§€ìš©: OCR bbox ì‚¬ìš© (ê°™ì€ í–‰ íŒë‹¨ì„ ìœ„í•´ ì›ë³¸ ì¢Œí‘œ ì‚¬ìš©)
         cell_bbox = (x, int(min(ys)), box_width, box_height)
+        # â˜… í…Œì´ë¸” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
+        in_table = is_inside_table(bbox, table_regions)
 
         text_render_info.append({
             'x': x, 'y': y, 'y_adjusted': y_adjusted,
@@ -1461,7 +1748,8 @@ def replace_text_in_image(image_path, translations, output_path):
             'text_width': text_width, 'text_height': actual_text_height,
             'cell_bbox': cell_bbox, 'bg_color': bg_color,
             'is_vertical': is_vertical, 'bbox': bbox,
-            'has_korean': item.get("has_korean", True)  # í•œê¸€ í¬í•¨ ì—¬ë¶€ í”Œë˜ê·¸
+            'has_korean': item.get("has_korean", True),  # í•œê¸€ í¬í•¨ ì—¬ë¶€ í”Œë˜ê·¸
+            'is_in_table': in_table  # â˜… í…Œì´ë¸” ë‚´ ì—¬ë¶€
         })
 
     # 3ë‹¨ê³„: ê²¹ì¹¨ ê°ì§€ - ì™¼ìª½ í…ìŠ¤íŠ¸ê°€ ì˜¤ë¥¸ìª½ ì…€ì„ ì¹¨ë²”í•˜ëŠ”ì§€ ì²´í¬
@@ -1528,7 +1816,7 @@ def replace_text_in_image(image_path, translations, output_path):
         display_text = info['text']
 
         if i in needs_abbreviation:
-            display_text = abbreviate_text(info['text'], used_abbreviations)
+            display_text = abbreviate_text(info['text'], used_abbreviations, target_lang)
 
         text_color = get_text_color_for_background(info['bg_color'])
         text_color_rgb = (text_color[2], text_color[1], text_color[0]) if text_color == (255, 255, 255) else text_color
@@ -1538,7 +1826,9 @@ def replace_text_in_image(image_path, translations, output_path):
                              text_color_rgb, info['cell_bbox'][2], info['cell_bbox'][3])
         else:
             # í´ë¦¬í•‘: í…ìŠ¤íŠ¸ë¥¼ ì„ì‹œ ì´ë¯¸ì§€ì— ê·¸ë¦° í›„ ì…€ ë†’ì´ë§Œí¼ë§Œ ì˜ë¼ì„œ ë¶™ì„
+            cell_left = info['cell_bbox'][0]
             cell_top = info['cell_bbox'][1]
+            cell_width = info['cell_bbox'][2]
             cell_height = info['cell_bbox'][3]
 
             # í…ìŠ¤íŠ¸ bbox ê³„ì‚° (ì¶©ë¶„í•œ ì—¬ë°±ì—ì„œ)
@@ -1569,8 +1859,20 @@ def replace_text_in_image(image_path, translations, output_path):
                 # í…ìŠ¤íŠ¸ê°€ ì…€ë³´ë‹¤ ì‘ìŒ â†’ ì…€ ì¤‘ì•™ì— ë°°ì¹˜
                 paste_y = cell_top + (cell_height - text_height_temp) // 2 + y_offset
 
+            # â˜… Xì¶• ì •ë ¬: í…Œì´ë¸” ì•ˆì´ë©´ ì¤‘ì•™, ì•„ë‹ˆë©´ ì™¼ìª½
+            if info.get('is_in_table', False):
+                # í…Œì´ë¸” ë‚´ í…ìŠ¤íŠ¸ â†’ ì¤‘ì•™ ì •ë ¬
+                original_center_x = cell_left + cell_width // 2
+                paste_x = original_center_x - text_width_temp // 2
+                # ì™¼ìª½ ê²½ê³„ ì œí•œ
+                if paste_x < cell_left:
+                    paste_x = cell_left
+            else:
+                # í…Œì´ë¸” ë°– í…ìŠ¤íŠ¸ â†’ ì™¼ìª½ ì •ë ¬
+                paste_x = info['x']
+
             # ì›ë³¸ ì´ë¯¸ì§€ì— ë¶™ì—¬ë„£ê¸°
-            img_result.paste(temp_img, (info['x'], paste_y), temp_img)
+            img_result.paste(temp_img, (paste_x, paste_y), temp_img)
 
         text_bbox_new = draw.textbbox((0, 0), display_text, font=info['font'])
         new_width = text_bbox_new[2] - text_bbox_new[0]
@@ -1586,13 +1888,22 @@ def replace_text_in_image(image_path, translations, output_path):
     return output_path
 
 
-def generate_preview_image(image_base64, translations):
+def generate_preview_image(image_base64, translations, target_lang='english'):
     """ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ ìƒì„± (ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬) - v1.8.0 (ê²¹ì¹¨ ê°ì§€ + ì•½ì–´)"""
     # base64 ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
     image_data = base64.b64decode(image_base64)
     nparr = np.frombuffer(image_data, np.uint8)
     img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
     height, width = img.shape[:2]
+    
+    # â˜… í…Œì´ë¸” ì˜ì—­ ê°ì§€ (ì¤‘ì•™ ì •ë ¬ ì ìš© ì—¬ë¶€ íŒë‹¨ìš©)
+    temp_img_path = os.path.join(UPLOAD_FOLDER, f"temp_table_detect_{id(image_base64)}.png")
+    cv2.imwrite(temp_img_path, img)
+    table_regions = detect_table_regions(temp_img_path)
+    try:
+        os.remove(temp_img_path)
+    except:
+        pass
 
     # 1ë‹¨ê³„: í•œê¸€ í…ìŠ¤íŠ¸ ì˜ì—­ë§Œ ë°°ê²½ìƒ‰ìœ¼ë¡œ ì§€ìš°ê¸° (ì˜ì–´ëŠ” ì›ë³¸ ìœ ì§€)
     bg_colors = {}
@@ -1658,6 +1969,8 @@ def generate_preview_image(image_base64, translations):
         is_vertical = is_vertical_text(bbox)
         # ê²¹ì¹¨ ê°ì§€ìš©: OCR bbox ì‚¬ìš© (ê°™ì€ í–‰ íŒë‹¨ì„ ìœ„í•´ ì›ë³¸ ì¢Œí‘œ ì‚¬ìš©)
         cell_bbox = (x, int(min(ys)), box_width, box_height)
+        # â˜… í…Œì´ë¸” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
+        in_table = is_inside_table(bbox, table_regions)
 
         text_render_info.append({
             'x': x, 'y': y, 'y_adjusted': y_adjusted,
@@ -1665,7 +1978,8 @@ def generate_preview_image(image_base64, translations):
             'text_width': text_width, 'text_height': actual_text_height,
             'cell_bbox': cell_bbox, 'bg_color': bg_color,
             'is_vertical': is_vertical, 'bbox': bbox,
-            'has_korean': item.get("has_korean", True)  # í•œê¸€ í¬í•¨ ì—¬ë¶€ í”Œë˜ê·¸
+            'has_korean': item.get("has_korean", True),  # í•œê¸€ í¬í•¨ ì—¬ë¶€ í”Œë˜ê·¸
+            'is_in_table': in_table  # â˜… í…Œì´ë¸” ë‚´ ì—¬ë¶€
         })
 
     # 3ë‹¨ê³„: ê²¹ì¹¨ ê°ì§€ - ì™¼ìª½ í…ìŠ¤íŠ¸ê°€ ì˜¤ë¥¸ìª½ ì…€ì„ ì¹¨ë²”í•˜ëŠ”ì§€ ì²´í¬
@@ -1735,7 +2049,7 @@ def generate_preview_image(image_base64, translations):
 
         # ì¹¨ë²”í•œ í…ìŠ¤íŠ¸ëŠ” ì•½ì–´ë¡œ ë³€í™˜
         if i in needs_abbreviation:
-            display_text = abbreviate_text(info['text'], used_abbreviations)
+            display_text = abbreviate_text(info['text'], used_abbreviations, target_lang)
 
         text_color = get_text_color_for_background(info['bg_color'])
         text_color_rgb = (text_color[2], text_color[1], text_color[0]) if text_color == (255, 255, 255) else text_color
@@ -1745,7 +2059,9 @@ def generate_preview_image(image_base64, translations):
                              text_color_rgb, info['cell_bbox'][2], info['cell_bbox'][3])
         else:
             # í´ë¦¬í•‘: í…ìŠ¤íŠ¸ë¥¼ ì„ì‹œ ì´ë¯¸ì§€ì— ê·¸ë¦° í›„ ì…€ ë†’ì´ë§Œí¼ë§Œ ì˜ë¼ì„œ ë¶™ì„
+            cell_left = info['cell_bbox'][0]
             cell_top = info['cell_bbox'][1]
+            cell_width = info['cell_bbox'][2]
             cell_height = info['cell_bbox'][3]
 
             # í…ìŠ¤íŠ¸ bbox ê³„ì‚° (ì¶©ë¶„í•œ ì—¬ë°±ì—ì„œ)
@@ -1776,8 +2092,20 @@ def generate_preview_image(image_base64, translations):
                 # í…ìŠ¤íŠ¸ê°€ ì…€ë³´ë‹¤ ì‘ìŒ â†’ ì…€ ì¤‘ì•™ì— ë°°ì¹˜
                 paste_y = cell_top + (cell_height - text_height_temp) // 2 + y_offset
 
+            # â˜… Xì¶• ì •ë ¬: í…Œì´ë¸” ì•ˆì´ë©´ ì¤‘ì•™, ì•„ë‹ˆë©´ ì™¼ìª½
+            if info.get('is_in_table', False):
+                # í…Œì´ë¸” ë‚´ í…ìŠ¤íŠ¸ â†’ ì¤‘ì•™ ì •ë ¬
+                original_center_x = cell_left + cell_width // 2
+                paste_x = original_center_x - text_width_temp // 2
+                # ì™¼ìª½ ê²½ê³„ ì œí•œ
+                if paste_x < cell_left:
+                    paste_x = cell_left
+            else:
+                # í…Œì´ë¸” ë°– í…ìŠ¤íŠ¸ â†’ ì™¼ìª½ ì •ë ¬
+                paste_x = info['x']
+
             # ì›ë³¸ ì´ë¯¸ì§€ì— ë¶™ì—¬ë„£ê¸°
-            img_result.paste(temp_img, (info['x'], paste_y), temp_img)
+            img_result.paste(temp_img, (paste_x, paste_y), temp_img)
 
         # bbox ê¸°ë¡
         text_bbox_new = draw.textbbox((0, 0), display_text, font=info['font'])
@@ -2335,8 +2663,9 @@ HTML_TEMPLATE = """
 
         /* ìš©ì–´ ì‚¬ì „ ëª¨ë‹¬ ìŠ¤íƒ€ì¼ */
         .dict-modal {
-            width: 700px;
-            max-height: 80vh;
+            width: 890px;
+            max-width: 95vw;
+            max-height: 99vh;
         }
         .dict-tabs {
             display: flex;
@@ -2365,9 +2694,11 @@ HTML_TEMPLATE = """
             display: flex;
             gap: 10px;
             margin-bottom: 15px;
+            flex-wrap: nowrap;
         }
         .dict-add-form input {
             flex: 1;
+            min-width: 120px;
             padding: 10px;
             border: 2px solid #ddd;
             border-radius: 8px;
@@ -2377,6 +2708,10 @@ HTML_TEMPLATE = """
             outline: none;
             border-color: #667eea;
         }
+        .dict-add-form button {
+            flex-shrink: 0;
+            white-space: nowrap;
+        }
         .dict-search {
             margin-bottom: 10px;
         }
@@ -2412,6 +2747,14 @@ HTML_TEMPLATE = """
             position: sticky;
             top: 0;
         }
+        .dict-table th:nth-child(1) { width: 25%; }  /* í•œê¸€ */
+        .dict-table th:nth-child(2) { width: 35%; }  /* ë²ˆì—­ */
+        .dict-table th:nth-child(3) { width: 20%; }  /* ì•½ì–´ */
+        .dict-table th:nth-child(4) { width: 20%; }  /* ì‘ì—… */
+        .dict-table .abbr-cell {
+            color: #666;
+            font-style: italic;
+        }
         .dict-table tr:hover {
             background: #f8f9fa;
         }
@@ -2557,6 +2900,7 @@ HTML_TEMPLATE = """
             <button type="button" class="lang-btn" data-lang="chinese">ğŸ‡¨ğŸ‡³ä¸­</button>
             <button type="button" class="lang-btn" data-lang="indonesian">ğŸ‡®ğŸ‡©ID</button>
             <button type="button" class="lang-btn" data-lang="bengali">ğŸ‡§ğŸ‡©BN</button>
+            <button type="button" class="lang-btn" data-lang="myanmar">ğŸ‡²ğŸ‡²MY</button>
             <button type="button" class="file-select-btn" id="fileSelectBtn">ğŸ“ íŒŒì¼ì„ íƒ</button>
             <button type="button" class="translate-btn" id="translateBtn" disabled>ğŸš€ ë²ˆì—­</button>
             <button type="button" class="dict-btn" id="dictBtn" title="ìš©ì–´ ì‚¬ì „ ê´€ë¦¬">ğŸ“–</button>
@@ -2632,10 +2976,12 @@ HTML_TEMPLATE = """
                         <button class="dict-tab" data-lang="chinese">ğŸ‡¨ğŸ‡³ ì¤‘êµ­ì–´</button>
                         <button class="dict-tab" data-lang="indonesian">ğŸ‡®ğŸ‡© ì¸ë„ë„¤ì‹œì•„ì–´</button>
                         <button class="dict-tab" data-lang="bengali">ğŸ‡§ğŸ‡© ë²µê³¨ì–´</button>
+                        <button class="dict-tab" data-lang="myanmar">ğŸ‡²ğŸ‡² ë¯¸ì–€ë§ˆì–´</button>
                     </div>
                     <div class="dict-add-form">
                         <input type="text" id="dictKorean" placeholder="í•œê¸€ ìš©ì–´">
                         <input type="text" id="dictTranslation" placeholder="ë²ˆì—­">
+                        <input type="text" id="dictAbbr" placeholder="ì•½ì–´ (ì„ íƒ)">
                         <button type="button" class="btn-primary" id="addTermBtn">â• ì¶”ê°€</button>
                     </div>
                     <div class="dict-search">
@@ -2647,6 +2993,7 @@ HTML_TEMPLATE = """
                                 <tr>
                                     <th>í•œê¸€</th>
                                     <th>ë²ˆì—­</th>
+                                    <th>ì•½ì–´</th>
                                     <th>ì‘ì—…</th>
                                 </tr>
                             </thead>
@@ -3145,28 +3492,39 @@ HTML_TEMPLATE = """
             showPreviewBtn.textContent = 'â³ ìƒì„±ì¤‘...';
 
             try {
+                console.log('[Preview Debug] Sending request to /generate_preview...');
+                console.log('[Preview Debug] image length:', page.image ? page.image.length : 'null');
+                console.log('[Preview Debug] translations count:', page.translations ? page.translations.length : 0);
+                
                 const response = await fetch('/generate_preview', {
                     method: 'POST',
                     headers: {'Content-Type': 'application/json'},
                     body: JSON.stringify({
                         image: page.image,
-                        translations: page.translations
+                        translations: page.translations,
+                        target_lang: targetLang.value
                     })
                 });
 
+                console.log('[Preview Debug] Response status:', response.status);
                 const data = await response.json();
+                console.log('[Preview Debug] Response data:', data.success, data.error || 'OK');
 
                 if (data.success) {
                     previewCache[pageIdx] = data.preview;
                     previewImg.src = 'data:image/png;base64,' + data.preview;
+                    console.log('[Preview Debug] Preview loaded successfully');
                 } else {
                     console.error('Preview generation failed:', data.error);
+                    alert('ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨: ' + data.error);
                     previewImg.src = 'data:image/png;base64,' + page.image;
                 }
             } catch (error) {
                 console.error('Preview error:', error);
+                alert('ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: ' + error.message);
                 previewImg.src = 'data:image/png;base64,' + page.image;
             } finally {
+                console.log('[Preview Debug] Finally block executed');
                 showPreviewBtn.classList.remove('loading');
                 showPreviewBtn.textContent = 'ğŸ”„ ë¯¸ë¦¬ë³´ê¸°';
             }
@@ -3288,6 +3646,12 @@ HTML_TEMPLATE = """
 
             status.className = 'status success';
             status.innerHTML = `âœ… í˜ì´ì§€ ${currentPage + 1} ë²ˆì—­ í™•ì •ë¨`;
+            
+            // ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œë©´ ì¦‰ì‹œ ê°±ì‹  (ìºì‹œ ë¬´íš¨í™” í›„)
+            if (isPreviewMode) {
+                delete previewCache[currentPage];  // ìºì‹œ ì‚­ì œ
+                showPreviewImage(currentPage, true);  // ê°•ì œ ìƒˆë¡œê³ ì¹¨
+            }
         });
 
         // ëª¨ë“  í˜ì´ì§€ ì¬ë²ˆì—­ (ì–¸ì–´ ë³€ê²½ ì‹œ)
@@ -3488,22 +3852,29 @@ HTML_TEMPLATE = """
             const searchTerm = dictSearch.value.toLowerCase();
 
             const entries = Object.entries(langDict)
-                .filter(([kr, trans]) =>
-                    kr.toLowerCase().includes(searchTerm) ||
-                    trans.toLowerCase().includes(searchTerm)
-                )
+                .filter(([kr, termData]) => {
+                    const full = typeof termData === 'object' ? termData.full : termData;
+                    const abbr = typeof termData === 'object' ? (termData.abbr || '') : '';
+                    return kr.toLowerCase().includes(searchTerm) ||
+                           full.toLowerCase().includes(searchTerm) ||
+                           abbr.toLowerCase().includes(searchTerm);
+                })
                 .sort((a, b) => a[0].localeCompare(b[0], 'ko'));
 
-            dictBody.innerHTML = entries.map(([korean, translation]) => `
+            dictBody.innerHTML = entries.map(([korean, termData]) => {
+                const full = typeof termData === 'object' ? termData.full : termData;
+                const abbr = typeof termData === 'object' ? (termData.abbr || '') : '';
+                return `
                 <tr data-korean="${korean}">
                     <td class="korean-cell">${korean}</td>
-                    <td class="trans-cell">${translation}</td>
+                    <td class="trans-cell">${full}</td>
+                    <td class="abbr-cell">${abbr}</td>
                     <td class="actions">
                         <button class="edit-btn" onclick="editTerm('${korean}')">âœï¸</button>
                         <button class="delete-btn" onclick="deleteTerm('${korean}')">ğŸ—‘ï¸</button>
                     </td>
                 </tr>
-            `).join('');
+            `}).join('');
 
             dictCount.textContent = `ì´ ${entries.length}ê°œ ìš©ì–´`;
         }
@@ -3515,6 +3886,7 @@ HTML_TEMPLATE = """
         addTermBtn.addEventListener('click', async () => {
             const korean = dictKorean.value.trim();
             const translation = dictTranslation.value.trim();
+            const abbr = document.getElementById('dictAbbr').value.trim();
 
             if (!korean || !translation) {
                 alert('í•œê¸€ ìš©ì–´ì™€ ë²ˆì—­ì„ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.');
@@ -3525,13 +3897,14 @@ HTML_TEMPLATE = """
                 const res = await fetch(`/api/dictionary/${currentDictLang}`, {
                     method: 'POST',
                     headers: { 'Content-Type': 'application/json' },
-                    body: JSON.stringify({ korean, translation })
+                    body: JSON.stringify({ korean, translation, abbr })
                 });
                 const data = await res.json();
 
                 if (data.success) {
                     dictKorean.value = '';
                     dictTranslation.value = '';
+                    document.getElementById('dictAbbr').value = '';
                     await loadDictionary();
                 } else {
                     alert('ì¶”ê°€ ì‹¤íŒ¨: ' + data.error);
@@ -3545,10 +3918,13 @@ HTML_TEMPLATE = """
         window.editTerm = function(korean) {
             const row = document.querySelector(`tr[data-korean="${korean}"]`);
             const transCell = row.querySelector('.trans-cell');
+            const abbrCell = row.querySelector('.abbr-cell');
             const actionsCell = row.querySelector('.actions');
             const currentTrans = transCell.textContent;
+            const currentAbbr = abbrCell.textContent;
 
-            transCell.innerHTML = `<input type="text" class="edit-input" value="${currentTrans}">`;
+            transCell.innerHTML = `<input type="text" class="edit-input edit-trans" value="${currentTrans}">`;
+            abbrCell.innerHTML = `<input type="text" class="edit-input edit-abbr" value="${currentAbbr}">`;
             actionsCell.innerHTML = `
                 <button class="save-btn" onclick="saveTerm('${korean}')">ğŸ’¾</button>
                 <button class="cancel-btn" onclick="renderDictTable()">âœ–ï¸</button>
@@ -3559,8 +3935,10 @@ HTML_TEMPLATE = """
         // ìš©ì–´ ì €ì¥
         window.saveTerm = async function(korean) {
             const row = document.querySelector(`tr[data-korean="${korean}"]`);
-            const input = row.querySelector('.edit-input');
-            const translation = input.value.trim();
+            const transInput = row.querySelector('.edit-trans');
+            const abbrInput = row.querySelector('.edit-abbr');
+            const translation = transInput.value.trim();
+            const abbr = abbrInput ? abbrInput.value.trim() : '';
 
             if (!translation) {
                 alert('ë²ˆì—­ì„ ì…ë ¥í•˜ì„¸ìš”.');
@@ -3571,7 +3949,7 @@ HTML_TEMPLATE = """
                 const res = await fetch(`/api/dictionary/${currentDictLang}/${encodeURIComponent(korean)}`, {
                     method: 'PUT',
                     headers: { 'Content-Type': 'application/json' },
-                    body: JSON.stringify({ translation })
+                    body: JSON.stringify({ translation, abbr })
                 });
                 const data = await res.json();
 
@@ -3657,7 +4035,7 @@ def get_progress():
 
 @app.route('/analyze', methods=['POST'])
 def analyze():
-    """íŒŒì¼ ì—…ë¡œë“œ + OCR + ì´ˆê¸° ë²ˆì—­"""
+    """íŒŒì¼ ì—…ë¡œë“œ + OCR + ì´ˆê¸° ë²ˆì—­ (ë°°ì¹˜ OCR + ë³‘ë ¬ ë²ˆì—­ ìµœì í™”)"""
     try:
         if 'file' not in request.files:
             return jsonify({"success": False, "error": "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"})
@@ -3690,46 +4068,50 @@ def analyze():
         session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
         temp_image_paths[session_id] = image_paths
 
-        # ê° í˜ì´ì§€ ë¶„ì„ (2ë‹¨ê³„: OCR ë¨¼ì €, ë²ˆì—­ì€ ë°°ì¹˜ë¡œ)
         pages = []
         total_pages = len(image_paths)
-        all_pages_data = []  # OCR ê²°ê³¼ ì„ì‹œ ì €ì¥
-
-        # ===== 1ë‹¨ê³„: ëª¨ë“  í˜ì´ì§€ OCR =====
-        for i, img_path in enumerate(image_paths):
-            print(f"[OCR {i+1}/{total_pages}] {img_path}", flush=True)
-
-            # â˜… ì§„í–‰ ìƒí™©: OCR
-            update_progress("OCR", i+1, total_pages, f"í˜ì´ì§€ {i+1}/{total_pages} OCR ì²˜ë¦¬ ì¤‘...")
+        all_pages_data = []
+        
+        import time  # ì„±ëŠ¥ ì¸¡ì •ìš©
 
-            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
+        # ===== 1ë‹¨ê³„: ë°°ì¹˜ OCR (ëª¨ë“  í˜ì´ì§€ í•œë²ˆì—) =====
+        update_progress("OCR", 1, total_pages, f"ì „ì²´ {total_pages}ê°œ í˜ì´ì§€ ì¼ê´„ OCR ì²˜ë¦¬ ì¤‘...")
+        print(f"[Batch OCR] Processing {total_pages} pages at once...", flush=True)
+        
+        ocr_start = time.time()
+        all_ocr_results = get_ocr_results_batch(image_paths)
+        ocr_time = time.time() - ocr_start
+        print(f"[TIMING] OCR took {ocr_time:.2f}s for {total_pages} pages", flush=True)
+        
+        # OCR ê²°ê³¼ì™€ ì´ë¯¸ì§€ ì •ë³´ ê²°í•©
+        for i, (img_path, texts) in enumerate(zip(image_paths, all_ocr_results)):
             with open(img_path, "rb") as f:
                 image_base64 = base64.b64encode(f.read()).decode()
-
-            # OCR
-            texts = get_ocr_results(img_path)
-            print(f"  Found {len(texts)} Korean texts", flush=True)
-
+            
             all_pages_data.append({
                 "page_idx": i,
                 "img_path": img_path,
                 "image_base64": image_base64,
                 "texts": texts
             })
+        
+        print(f"[Batch OCR] Complete - {sum(len(p['texts']) for p in all_pages_data)} total texts", flush=True)
 
-        # ===== 2ë‹¨ê³„: ë°°ì¹˜ ë²ˆì—­ (1íšŒ API í˜¸ì¶œ) =====
+        # ===== 2ë‹¨ê³„: ë²ˆì—­ (ì—”ì§„ë³„ ìµœì í™”) =====
         total_texts = sum(len(p["texts"]) for p in all_pages_data)
-        update_progress("ë²ˆì—­", 1, 1, f"ì „ì²´ {total_texts}ê°œ í…ìŠ¤íŠ¸ ì¼ê´„ ë²ˆì—­ ì¤‘... (1íšŒ API í˜¸ì¶œ)")
-        print(f"[Batch Translation] Total {total_texts} texts from {total_pages} pages", flush=True)
-
-        # Gemini ë°°ì¹˜ ë²ˆì—­ ì‚¬ìš© (Free Tier ìµœì í™”)
-        print(f"[Debug] Batch condition check: ai_engine=='{ai_engine}', api_key={bool(api_key)}, total_texts={total_texts}", flush=True)
-        print(f"[Debug] Will use batch? {ai_engine == 'gemini' and api_key and total_texts > 0}", flush=True)
+        translate_start = time.time()
+        
         if ai_engine == "gemini" and api_key and total_texts > 0:
+            # Gemini: ë°°ì¹˜ ë²ˆì—­ (1íšŒ API í˜¸ì¶œ)
+            update_progress("ë²ˆì—­", 1, 1, f"ì „ì²´ {total_texts}ê°œ í…ìŠ¤íŠ¸ ì¼ê´„ ë²ˆì—­ ì¤‘... (Gemini ë°°ì¹˜)")
+            print(f"[Gemini Batch] Total {total_texts} texts from {total_pages} pages", flush=True)
+            
             batch_input = [{"page_idx": p["page_idx"], "texts": p["texts"]} for p in all_pages_data]
             translations_by_page = translate_batch_with_gemini(batch_input, target_lang, api_key, model)
+            
+            translate_time = time.time() - translate_start
+            print(f"[TIMING] Gemini Batch Translation took {translate_time:.2f}s for {total_texts} texts", flush=True)
 
-            # ê²°ê³¼ ì¡°í•©
             for page_data in all_pages_data:
                 page_idx = page_data["page_idx"]
                 translations = translations_by_page.get(page_idx, [])
@@ -3739,8 +4121,31 @@ def analyze():
                     "translations": translations,
                     "confirmed": False
                 })
+                
+        elif ai_engine in ("claude", "openai") and api_key and total_texts > 0:
+            # Claude/OpenAI: ë³‘ë ¬ ë²ˆì—­ (ë™ì‹œ API í˜¸ì¶œ)
+            update_progress("ë²ˆì—­", 1, 1, f"ì „ì²´ {total_texts}ê°œ í…ìŠ¤íŠ¸ ë³‘ë ¬ ë²ˆì—­ ì¤‘... ({ai_engine.upper()} ë³‘ë ¬)")
+            print(f"[Parallel Translation] {ai_engine.upper()} - {total_pages} pages", flush=True)
+            
+            translations_by_page = translate_pages_parallel(
+                all_pages_data, target_lang, ai_engine, api_key, model, max_workers=3
+            )
+            
+            translate_time = time.time() - translate_start
+            print(f"[TIMING] {ai_engine.upper()} Parallel Translation took {translate_time:.2f}s for {total_texts} texts ({total_pages} pages)", flush=True)
+            
+            for page_data in all_pages_data:
+                page_idx = page_data["page_idx"]
+                translations = translations_by_page.get(page_idx, [])
+                pages.append({
+                    "image": page_data["image_base64"],
+                    "image_path": page_data["img_path"],
+                    "translations": translations,
+                    "confirmed": False
+                })
+                
         else:
-            # ê¸°ì¡´ ë°©ì‹: í˜ì´ì§€ë³„ ë²ˆì—­ (Ollama, OpenAI ë“±)
+            # Ollama ë“±: ìˆœì°¨ ë²ˆì—­ (ë¡œì»¬ ëª¨ë¸ì€ ë³‘ë ¬í™” ì´ì  ì ìŒ)
             for page_data in all_pages_data:
                 update_progress("ë²ˆì—­", page_data["page_idx"]+1, total_pages,
                                f"í˜ì´ì§€ {page_data['page_idx']+1}/{total_pages} - {len(page_data['texts'])}ê°œ í…ìŠ¤íŠ¸ ë²ˆì—­ ì¤‘...")
@@ -3833,8 +4238,9 @@ def generate_preview():
         data = request.get_json()
         image_base64 = data.get('image')
         translations = data.get('translations', [])
+        target_lang = data.get('target_lang', 'english')
 
-        print(f"[generate_preview] Received {len(translations)} translations")
+        print(f"[generate_preview] Received {len(translations)} translations, target_lang={target_lang}")
         for i, t in enumerate(translations[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
             print(f"  [{i}] bbox: {t.get('bbox', 'N/A')}, text: {t.get('text', 'N/A')[:20]}...")
 
@@ -3847,7 +4253,7 @@ def generate_preview():
 
         # ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ ìƒì„±
         print("[generate_preview] Calling generate_preview_image...")
-        preview_base64 = generate_preview_image(image_base64, translations)
+        preview_base64 = generate_preview_image(image_base64, translations, target_lang)
         print("[generate_preview] Preview generated successfully")
 
         return jsonify({
@@ -3887,7 +4293,7 @@ def generate():
                 # ì´ë¯¸ì§€ì— ë²ˆì—­ ì ìš©
                 output_filename = f"translated_{timestamp}_page{i+1}_{target_lang}.png"
                 output_path = os.path.join(OUTPUT_FOLDER, output_filename)
-                replace_text_in_image(temp_img_path, translations, output_path)
+                replace_text_in_image(temp_img_path, translations, output_path, target_lang)
                 output_files.append(output_filename)
             else:
                 print(f"  No translations, skipping...")
@@ -3951,7 +4357,7 @@ def translate():
                 # ì´ë¯¸ì§€ êµì²´
                 output_filename = f"translated_{timestamp}_page{i+1}_{target_lang}.png"
                 output_path = os.path.join(OUTPUT_FOLDER, output_filename)
-                replace_text_in_image(img_path, translations, output_path)
+                replace_text_in_image(img_path, translations, output_path, target_lang)
                 output_files.append(output_filename)
             else:
                 print(f"  No Korean text found, skipping...")
@@ -4000,13 +4406,14 @@ def get_dictionary_by_language(language):
 
 @app.route('/api/dictionary/<language>', methods=['POST'])
 def add_term(language):
-    """ìš©ì–´ ì¶”ê°€ (í•œê¸€: ë²ˆì—­)"""
+    """ìš©ì–´ ì¶”ê°€ (í•œê¸€: {full: ë²ˆì—­, abbr: ì•½ì–´})"""
     global GARMENT_DICT
     GARMENT_DICT = load_garment_dict()
 
     data = request.json
     korean = data.get('korean', '').strip()
     translation = data.get('translation', '').strip()
+    abbr = data.get('abbr', '').strip()
 
     if not korean or not translation:
         return jsonify({"error": "korean and translation are required"}), 400
@@ -4014,10 +4421,10 @@ def add_term(language):
     if language not in GARMENT_DICT:
         return jsonify({"error": f"Language '{language}' not found"}), 404
 
-    GARMENT_DICT[language][korean] = translation
+    GARMENT_DICT[language][korean] = {"full": translation, "abbr": abbr}
 
     if save_garment_dict(GARMENT_DICT):
-        return jsonify({"success": True, "korean": korean, "translation": translation})
+        return jsonify({"success": True, "korean": korean, "translation": translation, "abbr": abbr})
     return jsonify({"error": "Failed to save dictionary"}), 500
 
 @app.route('/api/dictionary/<language>/<korean>', methods=['PUT'])
@@ -4028,6 +4435,7 @@ def update_term(language, korean):
 
     data = request.json
     translation = data.get('translation', '').strip()
+    abbr = data.get('abbr', '').strip()
 
     if not translation:
         return jsonify({"error": "translation is required"}), 400
@@ -4038,10 +4446,10 @@ def update_term(language, korean):
     if korean not in GARMENT_DICT[language]:
         return jsonify({"error": f"Term '{korean}' not found"}), 404
 
-    GARMENT_DICT[language][korean] = translation
+    GARMENT_DICT[language][korean] = {"full": translation, "abbr": abbr}
 
     if save_garment_dict(GARMENT_DICT):
-        return jsonify({"success": True, "korean": korean, "translation": translation})
+        return jsonify({"success": True, "korean": korean, "translation": translation, "abbr": abbr})
     return jsonify({"error": "Failed to save dictionary"}), 500
 
 @app.route('/api/dictionary/<language>/<korean>', methods=['DELETE'])
